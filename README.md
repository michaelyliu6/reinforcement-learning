# ğŸ¤– Reinforcement Learning Implementations

Playing around with reinforcement learning algorithms - from the classics to modern deep RL. This repo walks through my implementations of various RL approaches, with detailed documentation and experimental results.

## ğŸ¯ Implementations

### ğŸ² Classical Methods
- **Multi-Armed Bandits**
  - Exploration vs exploitation trade-off analysis
  - Implementation of UCB, Îµ-greedy, and optimistic initialization
  - Custom environments for both stationary and non-stationary reward distributions
  - Empirical analysis of convergence rates

- **Tabular Methods**
  - Q-Learning and SARSA with convergence guarantees
  - Dynamic programming for policy evaluation and improvement
  - Implementation of various exploration strategies
  - Custom grid world environments with configurable dynamics

### ğŸ§  Deep Reinforcement Learning
- **Deep Q-Network (DQN)**
  - Experience replay for sample efficiency
  - Target networks for training stability
  - Prioritized replay buffer implementation
  - Comprehensive hyperparameter tuning analysis
  - Integration with classic control environments

- **Proximal Policy Optimization (PPO)**
  - Parallel environment processing for efficient training
  - Clipped surrogate objective implementation
  - Generalized Advantage Estimation (GAE)
  - Adaptive learning rate scheduling
  - Support for both discrete and continuous action spaces

## ğŸ› ï¸ Technical Implementation

### ğŸ—ï¸ Architecture Design
- Modular implementation separating agents, environments, and training loops
- Type-safe codebase with comprehensive static typing
- Vectorized operations for computational efficiency
- Custom environment wrappers extending OpenAI Gymnasium

### âš¡ Performance Optimization
- Parallel environment execution for PPO
- Efficient replay buffer implementation using NumPy
- Vectorized advantage calculation
- Optimized policy network architectures

### ğŸ“Š Experiment Tracking
- Integration with Weights & Biases for:
  - Hyperparameter optimization
  - Training metrics visualization
  - Model performance analysis
  - Learning curves and reward tracking

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ multi_armed_bandit_tabular/              # Foundational RL concepts
â”‚   â””â”€â”€ multiarmed_bandit_and_tabular_rl.ipynb
â”‚       - Multi-armed bandit implementations
â”‚       - Tabular Q-learning and SARSA
â”‚       - Dynamic programming methods
â”‚
â”œâ”€â”€ sarsa_q-learning_dqn/                    # Deep Q-Learning implementation
â”‚   â””â”€â”€ SARSA_Q-Learning_DQN.ipynb
â”‚       - DQN with experience replay
â”‚       - Target network implementation
â”‚       - Classic control environment integration
â”‚
â”œâ”€â”€ proximal_policy_optimization_ppo/        # Advanced policy optimization
â”‚   â””â”€â”€ ppo.ipynb
â”‚       - PPO implementation with parallel training
â”‚       - GAE computation
â”‚       - Policy and value network architectures
â”‚
â”œâ”€â”€ plot_utils.py                           # Plotting utilities
â”‚   - Visualization functions
â”‚   - Training curve plotting
â”‚
â””â”€â”€ utils.py                                # Shared utilities and environments
    - Custom environment implementations
    - Training helper functions
    - Common utility functions
```

## ğŸš€ Tech Stack

Built with modern ML tools:
- **ğŸ”¥ PyTorch**: Neural networks and autograd
- **ğŸ® OpenAI Gymnasium**: Environment simulation
- **ğŸ”¢ NumPy/Einops**: Efficient computation
- **ğŸ“ˆ Weights & Biases**: Experiment tracking
- **âœ¨ Type Hints**: Code reliability
- **ğŸ“Š Plotly**: Interactive visualizations

## ğŸƒâ€â™‚ï¸ Getting Started

```bash
pip install -r requirements.txt
```

Each notebook comes packed with:
- Theoretical background and math
- Implementation details with docs
- Experimental results and analysis
- Hyperparameter tuning studies
- Learning visualizations

## ğŸ’¡ Implementation Notes

The code is built with:
- Clean, readable implementations following theory
- Detailed documentation explaining concepts
- Efficient vectorized operations
- Reproducible results (fixed seeds)
- Flexible, extensible architectures

## ğŸ”® What's Next?

### ğŸš§ In Progress
- **RLHF (Reinforcement Learning from Human Feedback)**
  - Reward model training pipeline
  - Human feedback collection interface
  - Integration with existing PPO implementation

### ğŸ“ On the Roadmap
- **GPTO (Generalized Policy Trust Optimization)**
  - Enhanced trust region optimization
  - Multi-task policy adaptation
  - Improved sample efficiency mechanisms

### ğŸ’­ Future Ideas
- Distributed training for large-scale environments
- MuJoCo integration for robotics
- Offline RL algorithms
- Multi-agent RL extensions